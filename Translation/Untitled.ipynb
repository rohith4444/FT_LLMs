{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e8226704-8e9c-4735-b11e-30760881689d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_PROJECT\"]=\"mistral_lang_finetuning\"\n",
    "\n",
    "from enum import Enum\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import bitsandbytes as bnb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "from peft import get_peft_model, PeftModel, LoraConfig, TaskType\n",
    "from trl import SFTTrainer\n",
    "from transformers import (AutoModelForCausalLM, \n",
    "                          AutoTokenizer, \n",
    "                          BitsAndBytesConfig, \n",
    "                          TrainingArguments, \n",
    "                          pipeline, \n",
    "                          logging,\n",
    "                          set_seed)\n",
    "from sklearn.metrics import (accuracy_score, \n",
    "                             classification_report, \n",
    "                             confusion_matrix)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "seed = 42\n",
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2fbe8d23-5e46-49fc-afa5-f9a07a6aff9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name=\"mistralai/Mistral-7B-Instruct-v0.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1fabdf70-7353-4f95-a107-d98d4df7d5b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               Input  \\\n",
      "0                                 His legs are long.   \n",
      "1                Who taught Tom how to speak French?   \n",
      "2                       I swim in the sea every day.   \n",
      "3  Tom popped into the supermarket on his way hom...   \n",
      "4                             Smoke filled the room.   \n",
      "\n",
      "                                              Output  \\\n",
      "0                       అతని కాళ్ళు పొడవుగా ఉన్నాయి.   \n",
      "1            టామ్ ఫ్రెంచ్ మాట్లాడటం ఎలా నేర్పించారు?   \n",
      "2              నేను ప్రతి రోజు సముద్రంలో ఈత కొడతాను.   \n",
      "3  టామ్ కొంచెం పాలు కొనడానికి ఇంటికి వెళ్ళేటప్పుడ...   \n",
      "4                                పొగ గదిని నింపింది.   \n",
      "\n",
      "                                         Instruction  \n",
      "0  You are a helpful assistant who translates Eng...  \n",
      "1  You are a helpful assistant who translates Eng...  \n",
      "2  You are a helpful assistant who translates Eng...  \n",
      "3  You are a helpful assistant who translates Eng...  \n",
      "4  You are a helpful assistant who translates Eng...  \n"
     ]
    }
   ],
   "source": [
    "# Initialize lists to hold the English and Telugu sentences\n",
    "english_sentences = []\n",
    "telugu_sentences = []\n",
    "instruction= \"You are a helpful assistant who translates English statements, words, or lines into Telugu. Use your language skills to provide accurate and meaningful translations.\"\n",
    "\n",
    "# Open the file and read line by line\n",
    "with open('english_telugu_data.txt', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        # Split the line based on the delimiter '++++$++++'\n",
    "        parts = line.split('++++$++++')\n",
    "        if len(parts) == 2:  # Ensure that there are exactly two parts\n",
    "            english_sentences.append(parts[0].strip())\n",
    "            telugu_sentences.append(parts[1].strip())\n",
    "\n",
    "# Create a DataFrame with the English and Telugu sentences\n",
    "df = pd.DataFrame({\n",
    "    'Input': english_sentences,\n",
    "    'Output': telugu_sentences,\n",
    "    'Instruction': instruction\n",
    "})\n",
    "\n",
    "# Print the first few rows of the DataFrame\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c0ccd04d-ae24-4ecd-8890-39f4cc56456f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "155798\n"
     ]
    }
   ],
   "source": [
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "55e95c8f-dd68-498b-a421-aa09ec3b0a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['Input', 'Output', 'Instruction'],\n",
      "        num_rows: 155798\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Convert pandas DataFrame to a DatasetDict\n",
    "dataset_dict = DatasetDict({\n",
    "    'train': Dataset.from_pandas(df)\n",
    "})\n",
    "\n",
    "# Print the dataset_dict structure\n",
    "print(dataset_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cd7a31-f2b0-49f3-ac69-2c8d52fafb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "template = \"\"\"{% for message in messages %}\\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% if loop.last and add_generation_prompt %}{{'<|im_start|>assistant\\n' }}{% endif %}{% endfor %}\"\"\"\n",
    "tokenizer.chat_template = template\n",
    "\n",
    "def preprocess(samples):\n",
    "    batch = []\n",
    "    for system_prompt, function_desc, conversation in zip(samples[\"system_message\"], samples[\"function_description\"], samples[\"conversations\"]):\n",
    "        try:\n",
    "            function_desc_formatted = json.dumps(json.loads(f\"[{function_desc}]\"), indent=2, sort_keys=True)\n",
    "        except:\n",
    "            function_desc_formatted = f\"[{function_desc}]\"\n",
    "        system_message = {\"role\": \"system\", \"content\": f\"{system_prompt}\\nfunctions: {function_desc_formatted}\"}\n",
    "        conversation.insert(0, system_message)\n",
    "        batch.append(tokenizer.apply_chat_template(conversation, tokenize=False))\n",
    "    return {\"content\": batch}\n",
    "\n",
    "dataset = dataset.map(\n",
    "    preprocess,\n",
    "    batched=True,\n",
    "    remove_columns=dataset[\"train\"].column_names\n",
    ")\n",
    "dataset = dataset[\"train\"].train_test_split(0.1)\n",
    "print(dataset)\n",
    "print(dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d7af75-25b8-4ab8-affc-e422f7b28a64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed334da1-fc1f-414e-a816-969d85a43c4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92df4d6-4cd5-4866-9088-5cb27442d726",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e047edb1-9794-42a0-b385-2704e547347e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293eb47e-a516-4e64-b762-3d20e8a360eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
